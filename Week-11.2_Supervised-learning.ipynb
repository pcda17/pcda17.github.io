{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Supervised learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Download sample 'sports' and 'world' articles sets, then unzip.\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "!wget -N https://github.com/pcda17/pcda17.github.io/raw/master/week/11/nyt_world_11-19-2017.zip\n",
    "!unzip -o nyt_world_11-19-2017.zip\n",
    "\n",
    "!wget -N https://github.com/pcda17/pcda17.github.io/raw/master/week/11/nyt_sports_11-19-2017.zip\n",
    "!unzip -o nyt_sports_11-19-2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading 'world' articles as a list of strings\n",
    "\n",
    "os.chdir('/sharedfolder/nyt_world_11-19-2017/')\n",
    "\n",
    "nyt_world_texts = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    text_data = open(filename).read().replace('\\n', ' ')\n",
    "    nyt_world_texts.append(text_data)\n",
    "\n",
    "print(len(nyt_world_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading 'sports' articles as a list of strings\n",
    "\n",
    "os.chdir('/sharedfolder/nyt_sports_11-19-2017/')\n",
    "\n",
    "nyt_sports_texts = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    text_data = open(filename).read().replace('\\n', ' ')\n",
    "    nyt_sports_texts.append(text_data)\n",
    "\n",
    "print(len(nyt_sports_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide articles into training and test sets (reserving 8 articles for test set)\n",
    "\n",
    "nyt_world_train = nyt_world_texts[:-8]\n",
    "nyt_sports_train = nyt_sports_texts[:-8]\n",
    "\n",
    "nyt_world_test = nyt_world_texts[-8:]\n",
    "nyt_sports_test = nyt_sports_texts[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combing training data\n",
    "\n",
    "combined_texts = nyt_world_train + nyt_sports_train\n",
    "\n",
    "## Creating list of associated class values: \n",
    "## 0 for 'world', 1 for 'sports'\n",
    "y = [0]*len(nyt_world_train) + [1]*len(nyt_sports_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating vectorized training set using our combined sentence list\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a multinomial naive Bayes classifier\n",
    "## X is a combined list of 'world' and 'sports' vectors\n",
    "## y is a list of classes (0 or 1)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifying 'world' test set\n",
    "\n",
    "input_vector = vectorizer.transform(nyt_world_test) ## Converting a list of string to vector format established above\n",
    "\n",
    "classifier.predict(input_vector) ## Classifying each article in the list. '0' is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifying 'sports' test set\n",
    "\n",
    "input_vector = vectorizer.transform(nyt_sports_test) ## Converting a list of string to vector format established above\n",
    "\n",
    "classifier.predict(input_vector) ## Classifying each article in the list. '1' is correct.\n",
    "\n",
    "## We'll continue using this set of vectors in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## k-nearest neighbor classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn_classifier.fit(X, y)\n",
    "\n",
    "knn_classifier.predict(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "lr_classifier.fit(X, y)\n",
    "\n",
    "lr_classifier.predict(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machine (SVM) classifer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "svm_classifier.fit(X, y)\n",
    "\n",
    "svm_classifier.predict(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-layer perceptron classifier (a shallow neural network)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "mlp_classifier.fit(X, y)\n",
    "\n",
    "mlp_classifier.predict(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assignment: *\n",
    "    \n",
    "    Write some code that downloads a live New York Times page and classifies it as 'world' or 'sports'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!apt-get -y install libxml2-dev libxslt-dev \n",
    "!pip3 install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the 'newspaper' package to extract article text\n",
    "\n",
    "from newspaper import Article\n",
    "\n",
    "def url_to_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article_text = article.text.replace('\\n', ' ')\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2017/11/19/sports/patriots-beat-raiders-mexico.html'\n",
    "\n",
    "article_text = url_to_article_text(url)\n",
    "\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
